# token_is: Decoupled PPO with Token-level Importance Sampling
# Policy: 3 models (rollout, old, θ)
# PPO: ✅ (clipping between old and θ)
# Correction: Token-level IS (between rollout and old)
# Performance: Standard

use_tis: true
use_rs: false

# Token-level IS: per-token importance weights
tis_level: "token"
tis_mode: "truncate"

# Token-level bounds are less aggressive
tis_lower_bound: 0.5
tis_upper_bound: 2.0

# No rejection sampling
rs_level: "token"
rs_lower_bound: null
rs_upper_bound: null
rs_veto_threshold: null

# Batch normalization to stabilize training
tis_batch_normalize: false

