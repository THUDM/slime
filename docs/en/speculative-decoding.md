# Speculative Decoding ‚Äì Usage Guide

### Support Status

* ‚úÖ MTP layer: **inference only**, not training yet

  * ‚úÖ Models with a **native MTP layer**

    * ‚úÖ Mimo-7B-RL
    * üß™ DeepSeek-V3 / DeepSeek-R1
    * üß™ GLM-4.5
  * ‚è≥ Draft models **trained with SpecForge**
* ‚è≥ MTP layer **training**

  * üöß Add sequence packing support for the MTP layer in **Megatron**

### How to Use

Add the following flags to `SGLANG_ARGS`:

```
--sglang-speculative-algorithm EAGLE
--sglang-speculative-num-steps 3
--sglang-speculative-eagle-topk 1
--sglang-speculative-num-draft-tokens 4
```

For detailed parameter meanings and configuration, see SGLang‚Äôs speculative decoding [documentation](https://docs.sglang.ai/advanced_features/speculative_decoding.html).

### Known Issues

* In the **verify** phase of speculative decoding, there is a CUDA Graph **padding** bug that can surface as two kinds of errors: [SGLang #9521](https://github.com/sgl-project/sglang/issues/9521) and [SGLang #8336](https://github.com/sgl-project/sglang/issues/8336).

  * **Workarounds:**

    1. Increase `--sglang-cuda-graph-bs` to avoid CUDA Graph padding.
    2. Disable CUDA Graph padding via `--sglang-disable-cuda-graph-padding`.
    3. Disable CUDA Graph entirely (**not recommended**).
  * This issue exists with **fa3** and **FlashInfer**, so it‚Äôs **backend-agnostic**.
  * For debugging, try enabling Slime‚Äôs `--debug-rollout-only` to rule out effects from parameter updates or model offload.
  * The bug is **more severe inside RL frameworks** (vs. running SGLang alone) and often appears at the **start of a rollout**, likely related to large **batch size fluctuations** common in RL.
* FlashInfer‚Äôs speculative decoding has an additional CUDA Graph padding bug: [SGLang #9481](https://github.com/sgl-project/sglang/issues/9481).

  * **Workaround:** switch the attention backend with `--sglang-attention-backend fa3`.
